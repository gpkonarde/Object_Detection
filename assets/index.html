<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>document</title>
    <style>
      /* Global styles */
      body {
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        align-items: center;
        height: 100vh;
        background-color: #121212;
        flex-direction: column;
        color: white;
        font-family: Arial, sans-serif;
      }

      /* Hide webcam video feed */
      #webcam {
        display: none;
      }

      /* Canvas for displaying video and pose detection */
      canvas {
        border: 2px solid #fff;
        width: 90%;
        height: auto;
        position: relative;
        display: block;
        border-radius: 15px;
      }

      /* Controls container (buttons, info, etc.) */
      #controlsContainer {
        display: flex;
        flex-direction: column;
        align-items: center;
        gap: 15px;
        position: absolute;
        bottom: 5%;
        width: 90%;
        max-width: 400px;
        background-color: rgba(0, 0, 0, 0.8);
        padding: 20px;
        border-radius: 12px;
      }

      /* Info container for displaying detected keypoints */
      #infoContainer {
        max-height: 150px;
        overflow-y: auto;
        color: white;
        width: 100%;
        text-align: center;
      }

      #infoContainer ul {
        padding: 0;
        margin: 0;
        list-style: none;
        font-size: 14px;
      }

      /* Button styling */
      #captureBtn {
        padding: 15px 25px;
        background-color: #4caf50;
        color: white;
        font-size: 16px;
        border: none;
        border-radius: 8px;
        cursor: pointer;
        width: 100%;
        transition: background-color 0.3s;
      }

      #captureBtn:hover {
        background-color: #45a049;
      }

      /* Responsive styling for different screen sizes */
      @media (min-width: 768px) {
        canvas {
          width: 70%;
        }
        #controlsContainer {
          width: 70%;
          padding: 25px;
        }
        #captureBtn {
          font-size: 18px;
        }
      }

      @media (min-width: 1024px) {
        canvas {
          width: 50%;
        }
        #controlsContainer {
          width: 50%;
          padding: 30px;
        }
        #captureBtn {
          font-size: 20px;
        }
        #infoContainer {
          max-height: 200px;
        }
      }
    </style>
  </head>
  <body>
    <video id="webcam" width="640" height="480" autoplay></video>
    <canvas id="canvas"></canvas>

    <!-- Grouped Controls -->
    <div id="controlsContainer">
      <div id="infoContainer" class="hidden">
        <h2>Detected Objects:</h2>
        <ul id="detectedObjectsList"></ul>
      </div>
      <button id="captureBtn">Capture</button>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/pose"></script>
    <script>
      const video = document.getElementById('webcam');
      const canvas = document.getElementById('canvas');
      const ctx = canvas.getContext('2d');
      const captureButton = document.getElementById('captureBtn'); // Capture button
      const infoContainer = document.getElementById('infoContainer'); // Info container to show detected objects
      const detectedObjectsList = document.getElementById(
        'detectedObjectsList',
      );

      let detector;
      let keypointsData = [];

      // Setup the camera stream
      async function setupCamera() {
        const stream = await navigator.mediaDevices.getUserMedia({
          video: {facingMode: 'environment'},
        });
        video.srcObject = stream;
        return new Promise(resolve => {
          video.onloadedmetadata = () => {
            resolve(video);
          };
        });
      }
      // Load MoveNet model
      async function loadMoveNet() {
        detector = await poseDetection.createDetector(
          poseDetection.SupportedModels.BlazePose,
          {
            // runtime: 'tfjs',
            runtime: 'mediapipe',
            solutionPath: 'https://cdn.jsdelivr.net/npm/@mediapipe/pose',
            modelType: 'Heavy',
          },
        );
        console.log('BlazePose model loaded successfully');
        await setupCamera(); // Start the camera once the model is loaded
        video.play();
        video.style.display = 'none';
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        detectPose(); // Start detecting poses
      }

      // Draw keypoints on the main canvas
      function drawKeypoints(keypoints) {
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

        keypoints.forEach(keypoint => {
          if (keypoint.score > 0.5) {
            const {x, y, z} = keypoint;

            // Adjust size based on z coordinate (depth)
            const size = Math.max(5 - z / 1000000, 1); // Example scaling for visibility

            ctx.beginPath();
            ctx.arc(x, y, size, 0, 2 * Math.PI);
            ctx.fillStyle = 'red'; // Color can also be adjusted based on z
            ctx.fill();
          }
        });

        drawSkeleton(keypoints);
      }

      // Draw skeleton from the keypoints
      function drawSkeleton(keypoints) {
        const adjacentKeyPoints = poseDetection.util.getAdjacentPairs(
          poseDetection.SupportedModels.BlazePose, // Adjusted to use BlazePose
        );
        adjacentKeyPoints.forEach(([i, j]) => {
          const kp1 = keypoints[i];
          const kp2 = keypoints[j];
          if (kp1.score > 0.5 && kp2.score > 0.5) {
            ctx.beginPath();
            ctx.moveTo(kp1.x, kp1.y);
            ctx.lineTo(kp2.x, kp2.y);
            ctx.strokeStyle = 'blue'; // Skeleton color
            ctx.lineWidth = 2;
            ctx.stroke();
          }
        });
      }

      // Detect pose and update the canvas with keypoints and skeleton
      async function detectPose() {
        const poses = await detector.estimatePoses(video);
        if (poses.length > 0) {
          drawKeypoints(poses[0].keypoints); // Draw keypoints and skeleton on the canvas
        }
        //   document.getElementById("webcam").style.display = none;
        requestAnimationFrame(detectPose);
      }

      // Capture keypoints and display in the info container
      captureButton.addEventListener('click', async () => {
        const poses = await detector.estimatePoses(video);
        if (poses.length > 0) {
          const keypoints = poses[0].keypoints;
          keypoints.forEach(keypoint => {});
          // Clear previous content
          detectedObjectsList.innerHTML = '';

          // Iterate over the keypoints and add the body parts with their coordinates to the list
          keypoints.forEach(keypoint => {
            if (keypoint.score > 0.5) {
              const listItem = document.createElement('li');
              listItem.textContent = `${
                keypoint.name
              }: (x: ${keypoint.x.toFixed(2)}, y: ${keypoint.y.toFixed(2)})`;
              detectedObjectsList.appendChild(listItem);
            }
          });

          // Show the info container
          infoContainer.style.display = 'block';
        }
      });

      // Main function to set up the camera and load the model
      async function main() {
        await loadMoveNet();
      }

      main(); // Call the main function to start the app
    </script>
  </body>
</html>
